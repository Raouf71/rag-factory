# Step-by-Step Breakdown of the Pipeline


## RAG Pipeline Stages

Each domain goes through the full ingestion-to-answer pipeline:

- **Document Parsing** â€” PDF/HTML/CSV.<br>
Parsing can be done in two different ways: <br>
  * ``Parse into Document``:
    * Output = larger logical units (often per page or file)
    * Chunking happens **later** via your chunking strategy (chunk_size, overlap)
    * More flexible
    * Best when:
      * control over splitting is required
      * experimenting with chunk sizes
      * combined with structured extraction
      * hybrid/KG pipelines are built <br>

    ğŸ‘‰ Recommended for production RAG.
  * ``Parse into Node``:
    * Output = already pre-chunked pieces
    * Chunking happens **during parsing**
    * Less control afterward
    * Best when:
      * quick setup is required
      * parserâ€™s splitting is trusted
      * no custom chunk logic needed<br>

    ğŸ‘‰ Recommended for fast prototyping.

- **Splitting**<br>
semantic grouping of document pages:
  * e.g., 1 page = 1 mechanical part
  * preserves entity boundaries<br>
```(TODO)
ğŸ‘‰ Split before extraction, because extraction should **operate on a clean entity unit**.
    <br>
    
    If you donâ€™t split:
      * Extractor may mix multiple parts
      * Wrong DIN/material/torque assigned<br>

    If you split:
      * Correct metadata association
      * No cross-part contamination
```

- **Document Extracting**

  TWO-LAYER EXTRACTION ARCHITECTURE:
  * Layer 1 â€” ``PER_PAGE``
  Extract:
  - Part-level metadata (DIN, module, material, torque range, fits, tolerance class, description)

    Why:
    - Each page = one entity
    - Clean schema
    - Enables structured filtering + KG node creation
    - Stable and deterministic

  * Layer 2 - ``PER_TABLE_ROW``:
  Extract:
  - Each dimension/spec row separately
  (e.g., size variant, bore diameter, torque value, tolerance per row)

    Why:
    - Long tables cause LLM truncation in PER_DOC
    - Guarantees exhaustive row coverage
    - Enables exact numeric lookups without hallucination
    - Supports range filtering and comparison

  > Metadata extraction gives entity identity + filters (**â€œWho is the part?â€**) and row extraction gives precise engineering lookup (**â€œWhat are its exact dimension variants?â€**). 
  
  The architecture prevents:
    * Missing table rows
    * Numeric hallucinations
    * Mixed specs across parts

  (TODO) mapping layer 1 to layer 2

- **Chunking** â€” fixed-size, sentence-aware, and semantic chunking strategies
- **Embedding** â€” dense vector representations (e.g. `text-embedding-3-small`, `bge-m3`)
- **Indexing** â€” vector stores (FAISS, Qdrant, Weaviate, PostgreSQL) + graph stores for KG-RAG
- **Retrieval** â€” top-k similarity search, hybrid search, graph traversal, agent-driven retrieval

## Challenges of RAG for catalogs



## Pipeline Details

<details>
<summary>basic pipeline</summary>

---

## Step 1 â€” Parsing / Loading
**Raw PDF â†’ `Document` objects**

- Tool: `LlamaParse(...).load_data("../data/bevel_gear.pdf")`
- Output: a list of LlamaIndex `Document` objects (`docs`)

---

## Step 2 â€” Splitting
**`Document` objects â†’ smaller `Document` / Node chunks**

- Strategy: manual page-level split using `doc.text.split("\n---\n")`
- Each chunk is re-wrapped: `sub_doc = Document(text=..., metadata=...)`
- Output: `sub_docs` â€” a list of page-level `Document` objects

---

## Step 3 â€” Indexing (Vector)
**`sub_docs` â†’ searchable vector index**

- Index: `VectorStoreIndex.from_documents(sub_docs, embed_model=...)`
- Retriever: `base_index.as_retriever(similarity_top_k=10)`
- Query interface: `RetrieverQueryEngine`

---

## Step 4 â€” Knowledge Graph Extraction + Indexing
**Entity/relationship extraction â†’ graph store (Neo4j)**

- Graph store: `Neo4jPGStore`
- Graph index: `PropertyGraphIndex.from_documents(...)`
- KG extractors:
  - `ImplicitPathExtractor()` â€” implicit path inference
  - `SimpleLLMPathExtractor(llm=OpenAI(...), max_paths_per_chunk=..., num_workers=...)` â€” LLM-driven path extraction
- Embeddings for graph context retrieval: `OpenAIEmbedding("text-embedding-3-small")`

---

## Step 5 â€” Retrieval (Vector + KG â†’ Merged)
**Two retrievers â†’ fused results**

- Vector retriever: `vector_retriever = base_index.as_retriever(...)`
- KG retriever: `VectorContextRetriever(...)` â€” embedding similarity + graph expansion via `path_depth`
- Fusion: `CustomRetriever(BaseRetriever)` â€” merges results by `node_id`, deduplicating across both retrievers

</details>


<details>
<summary>advanced pipeline</summary>

---

## Step 1 â€” Parsing / Loading

---

## Step 2 â€” Splitting

---

## Step 3 â€” Indexing
---

## Step 4 â€” Knowledge Graph Extraction + Indexing

---

## Step 5 â€” Retrieval (Vector + KG â†’ Merged)

</details>

[ğŸ”¼ Back to top](#rag-pipeline-stages)
