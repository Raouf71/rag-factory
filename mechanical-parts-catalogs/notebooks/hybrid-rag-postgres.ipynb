{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f7051",
   "metadata": {},
   "source": [
    "## PoC for advanced RAG with PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d3724",
   "metadata": {},
   "source": [
    "___\n",
    "### Activate python virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bd314",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%source ~/path-to-your-project/llamaindex-venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fae8ca",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "## Setup Postgres and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f6725",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd964979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "def run_sudo(cmd, sudo_password, check=True):\n",
    "    \"\"\"Run a command with sudo -S, providing password via stdin.\"\"\"\n",
    "    return subprocess.run(\n",
    "        [\"sudo\", \"-S\"] + cmd,\n",
    "        input=(sudo_password + \"\\n\"),\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        check=check,\n",
    "        cwd=\"/tmp\",\n",
    "    )\n",
    "\n",
    "# --- passwords ---\n",
    "sudo_password = getpass.getpass(\"Provide sudo password: \")\n",
    "postgres_pw = getpass.getpass(\"Provide PostgreSQL password for user 'postgres': \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8789c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- system packages ---\n",
    "run_sudo([\"apt\", \"update\"], sudo_password)\n",
    "run_sudo([\"apt\", \"install\", \"-y\", \"postgresql-common\"], sudo_password)\n",
    "print(\"✅ system packages\")\n",
    "\n",
    "# Add PostgreSQL APT repo helper (from postgresql-common)\n",
    "run_sudo([\"/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh\"], sudo_password)\n",
    "print(\"✅ PostgreSQL APT repo helper\")\n",
    "\n",
    "# Install PostgreSQL + pgvector\n",
    "command = \"sudo -S apt install postgresql-15-pgvector\"\n",
    "os.system(f'echo \"{sudo_password}\" | {command}')\n",
    "# run_sudo([\"apt\", \"install\", \"-y\", \"postgresql\", \"postgresql-15-pgvector\"], sudo_password)\n",
    "print(\"✅ Install PostgreSQL + pgvector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd560c",
   "metadata": {},
   "source": [
    "## Start and enable PostgreSQL service:\n",
    "Ensures the DB server is running and starts automatically on reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc34c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ service is running\n",
      "ALTER ROLE\n",
      "✅ set postgres user password\n"
     ]
    }
   ],
   "source": [
    "# Ensure service is running\n",
    "run_sudo([\"systemctl\", \"enable\", \"--now\", \"postgresql\"], sudo_password)\n",
    "print(\"✅ service is running\")\n",
    "\n",
    "# --- set postgres user password ---\n",
    "sql_set_pw = f\"ALTER USER postgres WITH PASSWORD '{postgres_pw}';\"\n",
    "res = subprocess.run(\n",
    "    [\"sudo\", \"-S\", \"-u\", \"postgres\", \"psql\", \"-c\", sql_set_pw],\n",
    "    input=(sudo_password + \"\\n\"),\n",
    "    text=True,\n",
    "    check=True,\n",
    "    cwd=\"/tmp\",\n",
    ")\n",
    "# print(\"Return code:\", res.returncode)\n",
    "# print(\"STDOUT:\\n\", res.stdout)\n",
    "# print(\"STDERR:\\n\", res.stderr)\n",
    "print(\"✅ set postgres user password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c4a5b",
   "metadata": {},
   "source": [
    "## Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f34858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DO\n",
      "✅ create database\n"
     ]
    }
   ],
   "source": [
    "# --- create database (idempotent) ---\n",
    "sql_create_db = \"CREATE DATABASE vector_db;\"\n",
    "# If DB exists, CREATE DATABASE fails; so check first with psql:\n",
    "sql_create_db_safe = \"\"\"\n",
    "DO $$\n",
    "BEGIN\n",
    "   IF NOT EXISTS (SELECT FROM pg_database WHERE datname = 'vector_db') THEN\n",
    "      CREATE DATABASE vector_db;\n",
    "   END IF;\n",
    "END $$;\n",
    "\"\"\"\n",
    "subprocess.run(\n",
    "    [\"sudo\", \"-S\", \"-u\", \"postgres\", \"psql\", \"-c\", sql_create_db_safe],\n",
    "    input=(sudo_password + \"\\n\"),\n",
    "    text=True,\n",
    "    check=True,\n",
    "    cwd=\"/tmp\",\n",
    ")\n",
    "print(\"✅ create database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca360177",
   "metadata": {},
   "source": [
    "### Connect to vector_db and enable pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6bbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgvector extension: ('vector', '0.8.2')\n",
      "✅ PostgreSQL + pgvector ready. DB: vector_db, user: postgres\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# --- connect with psycopg2 to the new DB and enable pgvector extension ---\n",
    "connection_string=f\"postgresql://postgres:{postgres_pw}@localhost:5432\"\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "conn = psycopg2.connect(\n",
    "    dbname=db_name,\n",
    "    user=\"postgres\",\n",
    "    password=postgres_pw,\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    # c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    # c.execute(f\"CREATE DATABASE {db_name}\")\n",
    "    c.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    c.execute(\"SELECT extname, extversion FROM pg_extension WHERE extname='vector';\")\n",
    "    print(\"pgvector extension:\", c.fetchone())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ PostgreSQL + pgvector ready. DB: vector_db, user: postgres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1152113",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79290c04",
   "metadata": {},
   "source": [
    "# RAG pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c1809",
   "metadata": {},
   "source": [
    "### Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313141ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "if \"LLAMA_CLOUD_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass(\"Enter your Llama Cloud API Key: \")\n",
    "\n",
    "OPENAI_KEY = \"\"\n",
    "if OPENAI_KEY == \"\":\n",
    "    OPENAI_KEY = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943b7a8",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### Explicit model configuration\n",
    "Here we use gpt-4o and default OpenAI embeddings.\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm_model = OpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",  # 1536-dim\n",
    ")\n",
    "\n",
    "Settings.llm = llm_model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# --- Chunking defaults used by node parsers / ingestion ---\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 200\n",
    "\n",
    "# optional sanity prints\n",
    "print(\"LLM:\", Settings.llm)\n",
    "print(\"Embed model:\", Settings.embed_model)\n",
    "print(\"Chunk size/overlap:\", Settings.chunk_size, Settings.chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df9231",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 1. Parsing (``parse into document``)\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae61b60",
   "metadata": {},
   "source": [
    "##### Load and parse Data with agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import Document\n",
    "\n",
    "# PDF_PATH = \"../data/bevel_gear.pdf\"\n",
    "PDF_PATH = \"../data/gear_m2.pdf\"\n",
    "docs = LlamaParse(\n",
    "    parse_mode=\"parse_page_with_agent\",\n",
    "    # model=\"openai-gpt-4-1-mini\",\n",
    "    model=\"anthropic-sonnet-4.0\",      # strong layout + reasoning\n",
    "    high_res_ocr=True,                 # important for scanned PDFs\n",
    "    adaptive_long_table=True,          # preserves long tables\n",
    "    outlined_table_extraction=True,    # keeps header structure\n",
    "    output_tables_as_HTML=True,        # robust for row/column preservation\n",
    ").load_data(PDF_PATH)\n",
    "\n",
    "print(f\"Parsed pages: {len(docs)}\")\n",
    "print(\"Sample metadata:\", docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30997cf",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 2. Splitting (``manual split``)\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971b766",
   "metadata": {},
   "source": [
    "##### Split by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_page(parsed_docs):\n",
    "    sub_docs = []\n",
    "    for doc in parsed_docs:\n",
    "        page_chunks = doc.text.split(\"\\n---\\n\")\n",
    "        for i, chunk in enumerate(page_chunks):\n",
    "            md = deepcopy(doc.metadata)\n",
    "\n",
    "            # ensure page_number stays correct at page level\n",
    "            md[\"page_number\"] = md.get(\"page_number\", i + 1)\n",
    "\n",
    "            sub_docs.append(\n",
    "                Document(\n",
    "                    text=chunk,\n",
    "                    metadata=md,\n",
    "                )\n",
    "            )\n",
    "    return sub_docs\n",
    "\n",
    "sub_docs = split_by_page(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dae39",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 3.1 Extraction\n",
    "_______________________________\n",
    "\n",
    "TWO-LAYER EXTRACTION ARCHITECTURE:\n",
    "- ``Layer 1: PER_PAGE``  -> part-level metadata\n",
    "- ``Layer 2: PER_TABLE_ROW`` -> row-level dimension data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892409e5",
   "metadata": {},
   "source": [
    "#### Define the data schema for layer 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877efdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class PartSchema(BaseModel):\n",
    "    spur_gear_material: str = Field(description=\"The material from which the spur gear was manufactured (e.g. Steel, Stainless Steel, Plastics (Polyketon (PK), Polyacetal (POM)), etc.)\")\n",
    "    straight_toothed: bool = Field(description=\"It indicates whether, the teeth are aligned longitudinally with the shaft, meaning there is no \\\"helix angle\\\".\")\n",
    "    angle_of_engagement: int = Field(description=\"It refers to the angular position, or the arc, during which two gear teeth are in contact and transmitting power. It is often written in Degrees (°).\")\n",
    "    module: float = Field(description=\"The gear module of a gear represents the ratio of the pitch (distance between teeth) to pi (\\\\(\\\\pi \\\\)), effectively defining how thick a gear tooth is and, consequently, how strong it is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30697113",
   "metadata": {},
   "source": [
    "#### Define the data schema for layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ad6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableRowSchema(BaseModel):\n",
    "    ZZ: float = Field(description=\"ZZ (German for Zähnezahl) represents the number of teeth of the spur gear\")\n",
    "    ZB: float = Field(description=\"ZB (German for Zahn-breite) represents the width of the spur gear\")\n",
    "    ØB: float = Field(description=\"Represents the inner diameter of the spur gear\")\n",
    "    ØTK: float = Field(description=\"Represents the Pitch circle diameter of the spur gear\")\n",
    "    ØKK: float = Field(description=\"Represents the tip diameter of the spur gear\")\n",
    "    ØN: float = Field(description=\"Represents the Hub diameter of the spur gear\")\n",
    "    L: float = Field(description=\"The length of the spur gear\")\n",
    "    ØFM: float = Field(description=\"Represents the diameter of the ring gear\")\n",
    "    WS: float = Field(description=\"Represents the girder width\")\n",
    "    G: float = Field(description=\"The weight of the spur gear indicated in unit of gramms ([g]).\")\n",
    "    DM: float = Field(description=\"Represents the maximum permissible torque applied to the indicated in ([Ncm]).\")\n",
    "    art_nr: str = Field(description=\"'Art.-Nr.' is an abbreviation for the German term Artikelnummer, which translates to Article Number. It distinguishes a particular rack based on its specifications.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d3393",
   "metadata": {},
   "source": [
    "#### Run async extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services import (\n",
    "    LlamaExtract,\n",
    "    EU_BASE_URL,\n",
    ")\n",
    "from llama_cloud_services.extract import ExtractConfig, ExtractMode, ExtractTarget\n",
    "import asyncio\n",
    "\n",
    "async def run_extraction():\n",
    "\n",
    "    # Optionally, provide your project id, if not, it will use the 'Default' project\n",
    "    llama_extract = LlamaExtract(base_url=EU_BASE_URL)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # LAYER 1 — PER_PAGE (Part-level metadata)\n",
    "    # ---------------------------------------\n",
    "    part_results = await llama_extract.aextract(\n",
    "        data_schema=PartSchema,\n",
    "        files=[PDF_PATH],\n",
    "        config=ExtractConfig(\n",
    "            extraction_mode=ExtractMode.PREMIUM,\n",
    "            extraction_target=ExtractTarget.PER_PAGE,\n",
    "            parse_model=\"anthropic-sonnet-4.5\",\n",
    "            system_prompt=\"You are an expert at extracting specifications of spur gears from catalog documents\",\n",
    "        ),\n",
    "    )\n",
    "    # part_data       -> list[PartSchema]\n",
    "    print(f\"Extracted {len(part_results)} parts (PER_PAGE)\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # LAYER 2 — PER_TABLE_ROW (Dimension rows)\n",
    "    # ---------------------------------------\n",
    "    row_results = await llama_extract.aextract(\n",
    "        data_schema=TableRowSchema,\n",
    "        files=[PDF_PATH],\n",
    "        config=ExtractConfig(\n",
    "            extraction_mode=ExtractMode.PREMIUM,\n",
    "            extraction_target=ExtractTarget.PER_TABLE_ROW,\n",
    "            parse_model=\"anthropic-sonnet-4.5\",\n",
    "            system_prompt=\"You are an expert at extracting rows from dense dimension tables\",\n",
    "        ),\n",
    "    )\n",
    "    # table_row_data  -> list[TableRowSchema]\n",
    "    print(f\"Extracted {len(row_results)} table rows (PER_TABLE_ROW)\")\n",
    "\n",
    "    return part_results, row_results\n",
    "\n",
    "# Run async\n",
    "part_data, table_row_data = asyncio.run(run_extraction())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f2d6c",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 3.2. Mapping parts (Layer1) to table rows (Layer2):\n",
    "\n",
    "It guarantees:\n",
    "- Every dimension row belongs to exactly one part.\n",
    "- Structured Filtering + Precise Lookup\n",
    "- Build clean Knowledge Graph \n",
    "- Similar rows across parts don't confuse retrieval.\n",
    "- LLM doesn't wrong metadata.\n",
    "\n",
    "``Methodology``:\n",
    "- Step 1 — Choose the Linking Strategy (page_number or part_id)\n",
    "- Step 2 — Mapping Logic (Conceptually)\n",
    "- Step 3 — Deterministic Mapping\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18273c5",
   "metadata": {},
   "source": [
    "#### Link by page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lookup dictionary: page_number -> part\n",
    "parts_by_page = {\n",
    "    part.page_number: part\n",
    "    for part in part_data\n",
    "}\n",
    "\n",
    "# Attach rows to correct part\n",
    "for row in table_row_data:\n",
    "    page = row.page_number\n",
    "\n",
    "    if page in parts_by_page:\n",
    "        part = parts_by_page[page]\n",
    "\n",
    "        # Initialize container if needed\n",
    "        if not hasattr(part, \"dimension_rows\"):\n",
    "            part.dimension_rows = []\n",
    "\n",
    "        part.dimension_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642c1f5",
   "metadata": {},
   "source": [
    "#### Link by part_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_by_id = {\n",
    "    part.part_id: part \n",
    "    for part in part_data\n",
    "}\n",
    "\n",
    "parts_by_id = {part.part_id: part for part in part_data}\n",
    "\n",
    "for row in table_row_data:\n",
    "    if row.part_id in parts_by_id:\n",
    "        parts_by_id[row.part_id].dimension_rows.append(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
