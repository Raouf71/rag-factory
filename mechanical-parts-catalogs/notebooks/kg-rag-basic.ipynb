{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f7051",
   "metadata": {},
   "source": [
    "## PoC for advanced RAG patterns (naiveRAG, hybridRAG, GraphRAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7602e0",
   "metadata": {},
   "source": [
    "### Activate python virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1be787",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%source ~/path-to-your-project/llamaindex-venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02000905",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Import libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Parse\n",
    "from llama_cloud_services import LlamaParse\n",
    "from copy import deepcopy\n",
    "\n",
    "# Models\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# kg index\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "\n",
    "# Extractors\n",
    "from llama_index.core.indices.property_graph import (\n",
    "    ImplicitPathExtractor,\n",
    "    SimpleLLMPathExtractor,\n",
    ")\n",
    "\n",
    "# Custom retriever\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore, Document\n",
    "from typing import List\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.graph_stores.neo4j import Neo4jPGStore\n",
    "\n",
    "# agent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "# from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.core.agent.workflow import ToolCallResult, AgentStream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d2e5d",
   "metadata": {},
   "source": [
    "### Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303748ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "if \"LLAMA_CLOUD_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass(\"Enter your Llama Cloud API Key: \")\n",
    "\n",
    "OPENAI_KEY = \"\"\n",
    "if OPENAI_KEY == \"\":\n",
    "    OPENAI_KEY = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943b7a8",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### Setup Models\n",
    "Here we use gpt-4o and default OpenAI embeddings.\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = OpenAI(model=\"gpt-4o\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "Settings.llm = llm_model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Best chunk-configuration \n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 200\n",
    "print(Settings.context_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df9231",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 1. Parsing (``parse into document``)\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae61b60",
   "metadata": {},
   "source": [
    "##### Load and parse Data with agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = LlamaParse(\n",
    "    parse_mode=\"parse_page_with_agent\",\n",
    "    # model=\"openai-gpt-4-1-mini\",\n",
    "    model=\"anthropic-sonnet-4.0\",\n",
    "    high_res_ocr=True,\n",
    "    adaptive_long_table=True,\n",
    "    outlined_table_extraction=True,\n",
    "    output_tables_as_HTML=True,\n",
    ").load_data(\"../data/bevel_gear.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30997cf",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 2. Splitting (``manual split``)\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971b766",
   "metadata": {},
   "source": [
    "##### Split by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_docs(docs):\n",
    "    sub_docs = []\n",
    "    for doc in docs:\n",
    "        page_chunks = doc.text.split(\"\\n---\\n\")\n",
    "        for i, chunk in enumerate(page_chunks):\n",
    "            md = deepcopy(doc.metadata)\n",
    "\n",
    "            # ensure page_number stays correct at page level\n",
    "            md[\"page_number\"] = md.get(\"page_number\", i + 1)\n",
    "\n",
    "            sub_docs.append(\n",
    "                Document(\n",
    "                    text=chunk,\n",
    "                    metadata=md,\n",
    "                )\n",
    "            )\n",
    "    return sub_docs\n",
    "\n",
    "sub_docs = get_sub_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f9a6a",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 3. Indexing\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6fed6",
   "metadata": {},
   "source": [
    "##### 3.1 Vector-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex.from_documents(\n",
    "    sub_docs, \n",
    "    embed_model=embed_model,\n",
    "    vector_store='',  # if not specified, embeddings live in RAM\n",
    "    # vector_store=faiss_store,  # or Pinecone / Weaviate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6628cc8",
   "metadata": {},
   "source": [
    "##### 3.2 KG-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bba057",
   "metadata": {},
   "source": [
    "##### 3.2.1 Initialize Graph Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d4488",
   "metadata": {},
   "source": [
    "To launch Neo4j locally, first ensure you have docker installed. Then, you can launch the database with the following docker command:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    neo4j:latest\n",
    "```\n",
    "\n",
    "From here, you can open the db at [http://localhost:7474/](http://localhost:7474/). On this page, you will be asked to sign in. Use the default username/password of `neo4j` and `neo4j`.\n",
    "Once you login for the first time, you will be asked to change the password.\n",
    "\n",
    "After this, you are ready to create your first property graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d6f05",
   "metadata": {},
   "source": [
    "##### 3.2.2 Extract entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000da0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_graph_store_pw = getpass(\"Enter your Neo4j Graph Store Password: \")\n",
    "\n",
    "graph_store = Neo4jPGStore(\n",
    "    username=\"neo4j\",\n",
    "    password=neo4j_graph_store_pw,                   # your password\n",
    "    url=\"bolt://localhost:7687\",\n",
    ")\n",
    "vec_store = None\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    sub_docs,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    kg_extractors=[\n",
    "        ImplicitPathExtractor(),\n",
    "        SimpleLLMPathExtractor(\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "            num_workers=4,\n",
    "            max_paths_per_chunk=10,\n",
    "        ),\n",
    "    ],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    "    # vector_store=vector_store,\n",
    "    # embed_kg_nodes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265243c5",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "### 4. Retrieval\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c481113",
   "metadata": {},
   "source": [
    "##### 4.1 Vector retriever (embeddings similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = base_index.as_retriever(similarity_top_k=10)\n",
    "naive_query_engine = RetrieverQueryEngine(vector_retriever)\n",
    "\n",
    "# Query\n",
    "response = naive_query_engine.query(\n",
    "    \"Worum geht es in dem Dokument? Antworte in 2-3 S채tzen.\"\n",
    "    \"Aus welchem Material besteht das Kegelrad?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e9cb7",
   "metadata": {},
   "source": [
    "##### 4.2 Hybrid retriever (BM25 keyword + vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd495df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Retriever.from_documents(\n",
    "    sub_docs,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    retrievers=[bm25, vector_retriever],\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "hybrid_query_engine = RetrieverQueryEngine.from_args(retriever=hybrid_retriever)\n",
    "\n",
    "# Query\n",
    "response = hybrid_query_engine.query(\n",
    "    \"Worum geht es in dem Dokument? Antworte in 2-3 S채tzen.\"\n",
    "    \"Aus welchem Material besteht das Kegelrad?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95611e7f",
   "metadata": {},
   "source": [
    "##### 4.3 Knowledge graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_retriever = VectorContextRetriever(\n",
    "    index.property_graph_store,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    similarity_top_k=5,\n",
    "    path_depth=1,\n",
    "    # include_text=False,\n",
    "    include_text=True,\n",
    ")\n",
    "\n",
    "nodes = kg_retriever.retrieve(\n",
    "    \"Gib mir die ganze Reihe f체r den Kegelrad aus Zink mit M=2,0.\"\n",
    ")\n",
    "\n",
    "print(len(nodes))\n",
    "for idx, node in enumerate(nodes):\n",
    "    print(f\">> IDX: {idx}, {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbd7cf",
   "metadata": {},
   "source": [
    "#### 4.4 Custom retriever (vector+KG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both KG vector search and direct vector search.\"\"\"\n",
    "\n",
    "    def __init__(self, kg_retriever, vector_retriever):\n",
    "        self._kg_retriever = kg_retriever\n",
    "        self._vector_retriever = vector_retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        unique_nodes = {n.node_id: n for n in kg_nodes}\n",
    "        unique_nodes.update({n.node_id: n for n in vector_nodes})\n",
    "        return list(unique_nodes.values())\n",
    "custom_retriever = CustomRetriever(kg_retriever, vector_retriever)\n",
    "\n",
    "nodes = custom_retriever.retrieve(\n",
    "    \"Gib mir die ganze Reihe f체r den Kegelrad aus Zink mit M zwischen 1,0 und 2,0 und ZB=6,9 mm\"\n",
    ")\n",
    "\n",
    "print(len(nodes))\n",
    "for idx, node in enumerate(nodes):\n",
    "    print(f\">> IDX: {idx}, {node.get_content()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
