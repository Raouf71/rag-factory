{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f7051",
   "metadata": {},
   "source": [
    "# FAISS Vectore Store Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b6d88",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711ccbd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca606b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00811ef2",
   "metadata": {},
   "source": [
    "### Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"LLAMA_CLOUD_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass(\"Enter your Llama Cloud API Key: \")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84c4ed",
   "metadata": {},
   "source": [
    "### Setup faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e6a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# dimensions of text-ada-embedding-002/text-embedding-3-small\n",
    "d = 1536\n",
    "faiss_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12a409",
   "metadata": {},
   "source": [
    "### Load documents, build the VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2aabeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 12:19:30,340 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"/home/daghbeji/rag-factory/sandbox/vector-stores/data/\").load_data()\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820f63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 12:19:33,577 - INFO - Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "2026-02-21 12:19:33,578 - INFO - Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# save index to disk\n",
    "index.storage_context.persist()\n",
    "\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage_faiss\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage_faiss\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c6ee3",
   "metadata": {},
   "source": [
    "### Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29d3a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 12:18:49,523 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-21 12:18:50,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Two files are in the batch.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 12:18:50,822 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-21 12:18:51,784 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>A new framework called RAG-Anything is introduced to address the limitations of existing systems in handling multimodal knowledge representation and retrieval. It employs a dual-graph construction strategy to integrate visual elements, structured data, and textual knowledge seamlessly. By combining structural knowledge navigation with semantic similarity matching, RAG-Anything overcomes the shortcomings of traditional approaches and achieves superior performance, especially on long-context documents with evidence spanning multiple modalities.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"How many file are in our batch?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "response = query_engine.query(\"Summarize in 3-4 sentenecs.\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfbd220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 14:24:45,678 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-21 14:24:48,769 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The word \"RAG\" was used 18 times in the provided context. Here are the lines at which the word occurs:\n",
       "1. RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
       "2. Current RAG frameworks systematically exclude these vital knowledge sources across all three scenarios.\n",
       "3. Therefore, multimodal RAG emerges as a critical advancement.\n",
       "4. Addressing multimodal RAG presents three fundamental technical challenges that demand principled solutions.\n",
       "5. This makes it significantly more complex than traditional text-only approaches.\n",
       "6. The naive solution of converting all multimodal content to textual descriptions introduces severe information loss.\n",
       "7. These inherent limitations necessitate the design of effective technical components.\n",
       "8. Technical Challenges. â€¢ First, the unified multimodal representation challenge requires seamlessly integrating diverse information types.\n",
       "9. Second, the structure-aware decomposition challenge demands intelligent parsing of complex layouts.\n",
       "10. Third, the cross-modal retrieval challenge necessitates sophisticated mechanisms that can navigate between different modalities.\n",
       "11. Our Contributions. To address these challenges, we introduce RAG-Anything, a unified framework that fundamentally reimagines multimodal knowledge representation and retrieval.\n",
       "12. Rather than forcing diverse modalities into text-centric pipelines, RAG-Anything constructs complementary knowledge graphs that preserve both multimodal contextual relationships and detailed textual knowledge.\n",
       "13. The system maintains semantic integrity across modalities while ensuring efficient cross-modal reasoning capabilities throughout the process.\n",
       "14. Our cross-modal hybrid retrieval mechanism strategically combines structural knowledge navigation with semantic similarity matching.\n",
       "15. RAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns.\n",
       "16. The framework introduces modality-aware query processing and cross-modal alignment systems.\n",
       "17. This unified approach eliminates the architectural fragmentation that plagues current multimodal RAG systems.\n",
       "18. Our evaluation demonstrates that RAG-Anything achieves superior performance across diverse domains.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How many times was the word RAG used? Give me the lines at which the word occurs.\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d61616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
