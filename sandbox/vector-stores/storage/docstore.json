{"docstore/metadata": {"28d98ee8-8add-48f1-8310-b27d026e984d": {"doc_hash": "a0cdca74f25f9b91430bf7ae8fcec15e479c5cab1ff49138070a35a3b8cf36a8"}, "466fda6e-5b8e-4584-9040-f10551b13673": {"doc_hash": "06dbb66987283274e0ef8b2fd3429e5af3226fbb8284e5875d9e9102795adfe3", "ref_doc_id": "28d98ee8-8add-48f1-8310-b27d026e984d"}}, "docstore/ref_doc_info": {"28d98ee8-8add-48f1-8310-b27d026e984d": {"node_ids": ["466fda6e-5b8e-4584-9040-f10551b13673"], "metadata": {"page_label": "1", "file_name": "txt-only.pdf", "file_path": "/home/daghbeji/rag-factory/sandbox/vector-stores/data/txt-only.pdf", "file_type": "application/pdf", "file_size": 59025, "creation_date": "2026-02-21", "last_modified_date": "2026-02-21"}}}, "docstore/data": {"466fda6e-5b8e-4584-9040-f10551b13673": {"__data__": {"id_": "466fda6e-5b8e-4584-9040-f10551b13673", "embedding": null, "metadata": {"page_label": "1", "file_name": "txt-only.pdf", "file_path": "/home/daghbeji/rag-factory/sandbox/vector-stores/data/txt-only.pdf", "file_type": "application/pdf", "file_size": 59025, "creation_date": "2026-02-21", "last_modified_date": "2026-02-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28d98ee8-8add-48f1-8310-b27d026e984d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "txt-only.pdf", "file_path": "/home/daghbeji/rag-factory/sandbox/vector-stores/data/txt-only.pdf", "file_type": "application/pdf", "file_size": 59025, "creation_date": "2026-02-21", "last_modified_date": "2026-02-21"}, "hash": "a0cdca74f25f9b91430bf7ae8fcec15e479c5cab1ff49138070a35a3b8cf36a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\nneed for multimodal RAG capabilities. InScientific Research, experimental results are primarily\ncommunicated through plots, diagrams, and statistical visualizations. These contain core discoveries\nthat remain invisible to text-only systems.Financial Analysisrelies heavily on market charts,\ncorrelation matrices, and performance tables. Investment insights are encoded in visual patterns\nrather than textual descriptions. Additionally,Medical Literature Analysisdepends on radiological\nimages, diagnostic charts, and clinical data tables. These contain life-critical information essential for\naccurate diagnosis and treatment decisions. Current RAG frameworks systematically exclude these\nvital knowledge sources across all three scenarios. This creates fundamental gaps that render them\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,\nmultimodal RAG emerges as a critical advancement. It is necessary to bridge these knowledge gaps\nand enable truly comprehensive intelligence across all modalities of human knowledge representation.\nAddressing multimodal RAG presents three fundamental technical challenges that demand principled\nsolutions. This makes it significantly more complex than traditional text-only approaches. The naive\nsolution of converting all multimodal content to textual descriptions introduces severe information\nloss. Visual elements such as charts, diagrams, and spatial layouts contain semantic richness that\ncannot be adequately captured through text alone. These inherent limitations necessitate the design of\neffective technical components. Such components must be specifically designed to handle multimodal\ncomplexity and preserve the full spectrum of information contained within diverse content types.\nTechnical Challenges. \u2022 First, theunified multimodal representationchallenge requires seam-\nlessly integrating diverse information types. The system must preserve their unique characteristics\nand cross-modal relationships. This demands advanced multimodal encoders that can capture both\nintra-modal and inter-modal dependencies without losing essential visual semantics. \u2022 Second, the\nstructure-aware decompositionchallenge demands intelligent parsing of complex layouts. The\nsystem must maintain spatial and hierarchical relationships crucial for understanding. This requires\nspecialized layout-aware parsing modules that can interpret document structure and preserve contex-\ntual positioning of multimodal elements. \u2022 Third, thecross-modal retrievalchallenge necessitates\nsophisticated mechanisms that can navigate between different modalities. These mechanisms must\nreason over their interconnections during retrieval. This calls for cross-modal alignment systems\ncapable of understanding semantic correspondences across text, images, and structured data. These\nchallenges are amplified in long-context scenarios. Relevant evidence is dispersed across multiple\nmodalities and sections, requiring coordinated reasoning across heterogeneous information sources.\nOur Contributions. To address these challenges, we introduce RAG-Anything, a unified framework\nthat fundamentally reimagines multimodal knowledge representation and retrieval. Our approach\nemploys adual-graph construction strategythat elegantly bridges the gap between cross-modal\nunderstanding and fine-grained textual semantics. Rather than forcing diverse modalities into text-\ncentric pipelines, RAG-Anything constructscomplementary knowledge graphsthat preserve both\nmultimodal contextual relationships and detailed textual knowledge. This design enables seamless\nintegration of visual elements, structured data, and mathematical expressions within a unified retrieval\nframework. The system maintainssemantic integrityacross modalities while ensuring efficient\ncross-modal reasoning capabilitiesthroughout the process.\nOurcross-modal hybrid retrievalmechanism strategically combinesstructural knowledge nav-\nigationwithsemantic similarity matching. This architecture addresses the fundamental limita-\ntion of existing approaches that rely solely on embedding-based retrieval or keyword matching.\nRAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns. It\nsimultaneously employs dense vector representations to identify semantically relevant content that\nlacks direct structural connections. The framework introducesmodality-aware query processing\nandcross-modal alignment systems. These enable textual queries to effectively access visual and\nstructured information. This unified approach eliminates the architectural fragmentation that plagues\ncurrent multimodal RAG systems. It delivers superior performance particularly on long-context\ndocuments where relevant evidence spans multiple modalities and document sections.\nExperimental Validation. To validate the effectiveness of our proposed approach, we conduct com-\nprehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench.\nOur evaluation demonstrates that RAG-Anything achieves superior performance across diverse do-\nmains. The framework represents substantial improvements over state-of-the-art baselines. Notably,\nour performance gains become increasingly significant as content length increases. We observe\nparticularly pronounced advantages on long-context materials. This validates our core hypothesis\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}