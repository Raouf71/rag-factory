{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1f7051",
   "metadata": {},
   "source": [
    "# PostgreSQL Vectore Store (exploring **vector-** and **hybrid** search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b6d88",
   "metadata": {},
   "source": [
    "## Setup Postgres and Dependencies:\n",
    "- PostgreSQL tooling \n",
    "- Official PG repo helper\n",
    "- pgvector for PostgreSQL 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711ccbd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f406d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "def run_sudo(cmd, sudo_password, check=True):\n",
    "    \"\"\"Run a command with sudo -S, providing password via stdin.\"\"\"\n",
    "    return subprocess.run(\n",
    "        [\"sudo\", \"-S\"] + cmd,\n",
    "        input=(sudo_password + \"\\n\"),\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        check=check,\n",
    "        cwd=\"/tmp\",\n",
    "    )\n",
    "\n",
    "# --- passwords ---\n",
    "sudo_password = getpass.getpass(\"Provide sudo password: \")\n",
    "postgres_pw = getpass.getpass(\"Provide PostgreSQL password for user 'postgres': \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- system packages ---\n",
    "run_sudo([\"apt\", \"update\"], sudo_password)\n",
    "run_sudo([\"apt\", \"install\", \"-y\", \"postgresql-common\"], sudo_password)\n",
    "print(\"✅ system packages\")\n",
    "\n",
    "# Add PostgreSQL APT repo helper (from postgresql-common)\n",
    "run_sudo([\"/usr/share/postgresql-common/pgdg/apt.postgresql.org.sh\"], sudo_password)\n",
    "print(\"✅ PostgreSQL APT repo helper\")\n",
    "\n",
    "# Install PostgreSQL + pgvector\n",
    "command = \"sudo -S apt install postgresql-15-pgvector\"\n",
    "os.system(f'echo \"{sudo_password}\" | {command}')\n",
    "# run_sudo([\"apt\", \"install\", \"-y\", \"postgresql\", \"postgresql-15-pgvector\"], sudo_password)\n",
    "print(\"✅ Install PostgreSQL + pgvector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13eebfe",
   "metadata": {},
   "source": [
    "## Start and enable PostgreSQL service:\n",
    "Ensures the DB server is running and starts automatically on reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0daad53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ service is running\n",
      "ALTER ROLE\n",
      "✅ set postgres user password\n"
     ]
    }
   ],
   "source": [
    "# Ensure service is running\n",
    "run_sudo([\"systemctl\", \"enable\", \"--now\", \"postgresql\"], sudo_password)\n",
    "print(\"✅ service is running\")\n",
    "\n",
    "# --- set postgres user password ---\n",
    "sql_set_pw = f\"ALTER USER postgres WITH PASSWORD '{postgres_pw}';\"\n",
    "res = subprocess.run(\n",
    "    [\"sudo\", \"-S\", \"-u\", \"postgres\", \"psql\", \"-c\", sql_set_pw],\n",
    "    input=(sudo_password + \"\\n\"),\n",
    "    text=True,\n",
    "    check=True,\n",
    "    cwd=\"/tmp\",\n",
    ")\n",
    "# print(\"Return code:\", res.returncode)\n",
    "# print(\"STDOUT:\\n\", res.stdout)\n",
    "# print(\"STDERR:\\n\", res.stderr)\n",
    "print(\"✅ set postgres user password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4bed1",
   "metadata": {},
   "source": [
    "## Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27986f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DO\n",
      "✅ create database\n"
     ]
    }
   ],
   "source": [
    "# --- create database (idempotent) ---\n",
    "sql_create_db = \"CREATE DATABASE vector_db;\"\n",
    "# If DB exists, CREATE DATABASE fails; so check first with psql:\n",
    "sql_create_db_safe = \"\"\"\n",
    "DO $$\n",
    "BEGIN\n",
    "   IF NOT EXISTS (SELECT FROM pg_database WHERE datname = 'vector_db') THEN\n",
    "      CREATE DATABASE vector_db;\n",
    "   END IF;\n",
    "END $$;\n",
    "\"\"\"\n",
    "subprocess.run(\n",
    "    [\"sudo\", \"-S\", \"-u\", \"postgres\", \"psql\", \"-c\", sql_create_db_safe],\n",
    "    input=(sudo_password + \"\\n\"),\n",
    "    text=True,\n",
    "    check=True,\n",
    "    cwd=\"/tmp\",\n",
    ")\n",
    "print(\"✅ create database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00811ef2",
   "metadata": {},
   "source": [
    "### Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc17494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# if \"LLAMA_CLOUD_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass(\"Enter your Llama Cloud API Key: \")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84c4ed",
   "metadata": {},
   "source": [
    "### Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a28010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 001ecede-d1b4-443b-bdb4-6bec7d8b5829\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import textwrap\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfdd36",
   "metadata": {},
   "source": [
    "### Connect to vector_db and enable pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e6a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgvector extension: ('vector', '0.8.2')\n",
      "✅ PostgreSQL + pgvector ready. DB: vector_db, user: postgres\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# --- connect with psycopg2 to the new DB and enable pgvector extension ---\n",
    "connection_string=f\"postgresql://postgres:{postgres_pw}@localhost:5432\"\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "conn = psycopg2.connect(\n",
    "    dbname=db_name,\n",
    "    user=\"postgres\",\n",
    "    password=postgres_pw,\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    # c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    # c.execute(f\"CREATE DATABASE {db_name}\")\n",
    "    c.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    c.execute(\"SELECT extname, extversion FROM pg_extension WHERE extname='vector';\")\n",
    "    print(\"pgvector extension:\", c.fetchone())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ PostgreSQL + pgvector ready. DB: vector_db, user: postgres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e2798",
   "metadata": {},
   "source": [
    "## 1. VECTOR SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12a409",
   "metadata": {},
   "source": [
    "### Create the vector index\n",
    "The example below:\n",
    "1. generates embeddings for each document.\n",
    "2. stores them in PostgreSQL.\n",
    "3. creates a vector index (HNSW) on the embedding column with ``m = 16``, ``ef_construction = 64``, and ``ef_search = 40``, with the **vector_cosine_ops** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d61616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "\n",
    "# Create PGVectorStore instance\n",
    "url = make_url(connection_string)\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,               # tells LlamaIndex to use PostgreSQL as the backend\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=\"paul_graham_essay\", # is where chunks + embeddings + metadata will be stored\n",
    "    embed_dim=1536,                 # openai embedding dimension\n",
    "    hnsw_kwargs={                   # configure an HNSW ANN index using cosine distance\n",
    "        \"hnsw_m\": 16,\n",
    "        \"hnsw_ef_construction\": 64,\n",
    "        \"hnsw_ef_search\": 40,\n",
    "        \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Bind index to Postgres table (table_name) \n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Use existing index OR Build the RAG index from documents (parse+split+embed+populate table_name)\n",
    "# TODO: Implement ingestion guard to avoid re-processing UNCHANGED docs\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, \n",
    "#     storage_context=storage_context, \n",
    "#     show_progress=True\n",
    "# )\n",
    "\n",
    "# Embed user query + retrieve top-k chunks + send context to LLM to generate answer\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3537a3",
   "metadata": {},
   "source": [
    "### Query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc041c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do?\")\n",
    "print(textwrap.fill(str(response), 100))\n",
    "response = query_engine.query(\"What happened in the mid 1980s?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ef715",
   "metadata": {},
   "source": [
    "## 2. HYBRID SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bade6d",
   "metadata": {},
   "source": [
    "#### Create the hybrid index (vector + BM25)\n",
    "* Set ``hybrid_search=True`` <br>\n",
    "* Change ``text_search_config=\"english\"``<br>\n",
    "* Set ``vector_store_query_mode=\"hybrid\"``<br>\n",
    "* Adjust ``sparse_top_k`` (Default is \"similarity_top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "\n",
    "url = make_url(connection_string)\n",
    "hybrid_vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=\"paul_graham_essay_hybrid_search\",\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    "    hybrid_search=True,\n",
    "    text_search_config=\"english\",\n",
    "    hnsw_kwargs={\n",
    "        \"hnsw_m\": 16,\n",
    "        \"hnsw_ef_construction\": 64,\n",
    "        \"hnsw_ef_search\": 40,\n",
    "        \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "    },\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=hybrid_vector_store\n",
    ")\n",
    "\n",
    "# Create index\n",
    "# TODO: Implement ingestion guard to avoid re-processing UNCHANGED docs\n",
    "hybrid_index = VectorStoreIndex.from_vector_store(vector_store=hybrid_vector_store)\n",
    "# hybrid_index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66eb334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 14:49:06,409 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 14:49:07,557 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Painting students\n"
     ]
    }
   ],
   "source": [
    "hybrid_query_engine = hybrid_index.as_query_engine(\n",
    "    vector_store_query_mode=\"hybrid\", sparse_top_k=3\n",
    ")\n",
    "\n",
    "hybrid_response = hybrid_query_engine.query(\n",
    "    \"Who does Paul Graham think of with the word schtick\"\n",
    ")\n",
    "print(hybrid_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5074033",
   "metadata": {},
   "source": [
    "## Improving hybrid search with ``QueryFusionRetriever``:\n",
    "\n",
    "NOTE: score values from vector and sparse are on different scales; **mode=\"relative_score\"** normalizes/fuses them. If combined naively, one may dominate where the other may be underevaluated and ranked too low. That's why we do **smart score fusion**(agreement between methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59efb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Check following link for documentation (VectorStoreQueryMode)\n",
    "# https://developers.llamaindex.ai/python/framework-api-reference/storage/vector_store/?utm_source=chatgpt.com\n",
    "vector_retriever = hybrid_index.as_retriever(\n",
    "    vector_store_query_mode=\"default\",      # dense vector search (KNN over embeddings). It uses embedding column + distance operator (cosine)\n",
    "    similarity_top_k=5, \n",
    ")\n",
    "text_retriever = hybrid_index.as_retriever(\n",
    "    vector_store_query_mode=\"sparse\",       # sparse / keyword-style retrieval (token-based / BM25)\n",
    "    similarity_top_k=5,  # interchangeable with sparse_top_k in this context\n",
    ")\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, text_retriever],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"relative_score\",  \n",
    "    use_async=False,\n",
    ")\n",
    "\n",
    "response_synthesizer = CompactAndRefine()\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56ac8b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 14:50:33,842 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 14:50:35,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham thinks of a signature style when he uses the word \"schtick.\" He refers to a signature style as the visual equivalent of what in show business is known as a \"schtick,\" something that immediately identifies the work as yours and no one else's.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who does Paul Graham think of with the word schtick, and why?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e5b1d",
   "metadata": {},
   "source": [
    "### See each retriever’s results + scores (vector vs text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "qb = QueryBundle(\"Who does Paul Graham think of with the word schtick, and why?\")\n",
    "\n",
    "vec_nodes  = vector_retriever.retrieve(qb)\n",
    "text_nodes = text_retriever.retrieve(qb)\n",
    "\n",
    "print(\"=== VECTOR ===\")\n",
    "for n in vec_nodes:\n",
    "    # print(n.score, n.node.node_id, n.node.get_text()[:120])\n",
    "    print(n.score, n.node.get_text()[:120])\n",
    "\n",
    "print(\"\\n=== TEXT/SPARSE ===\")\n",
    "for n in text_nodes:\n",
    "    # print(n.score, n.node.node_id, n.node.get_text()[:120])\n",
    "    print(n.score, n.node.get_text()[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfbd88",
   "metadata": {},
   "source": [
    "___\n",
    "* Vector scores are typically ``in [0, 1]`` (cosine-ish)\n",
    "* Sparse/BM25-like scores are ``often small decimals`` (or sometimes larger numbers) depending on normalization.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f906e",
   "metadata": {},
   "source": [
    "### See the fused (final) list + fused scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_nodes = retriever.retrieve(qb)\n",
    "\n",
    "print(\"=== FUSED ===\")\n",
    "for n in fused_nodes:\n",
    "    # print(n.score, n.node.node_id, n.node.get_text()[:120])\n",
    "    print(n.score, n.node.get_text()[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8014f9",
   "metadata": {},
   "source": [
    "___\n",
    "* Node 8862... appears in **BOTH lists** and is **ranked #1** in both → strong signal it’s relevant (good candidate for fusion).\n",
    "* The other TEXT/SPARSE hits (c2ee..., 5b90...) contain query terms more literally (or match common words), but are not semantically close enough to appear in the vector top-k.\n",
    "* The other VECTOR hits (825e..., 773d...) are semantically related to the “Paul Graham essay” context, even if they don’t share exact keywords.\n",
    "___\n",
    "The objective of ``QueryFusionRetriever`` is:\n",
    "-> Balance ``literal keyword matche`` vs ``meaning matches`` without being fooled by the different score scales.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
